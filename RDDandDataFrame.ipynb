{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589192773996)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(# scalaSpark)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sourceFile: String = C:/workspace/workplace/scala/SparkScala/fakefriends.csv\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sourceFile = \"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[1] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(sourceFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[String] = Array(0,Will,33,385)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[Array[String]] = Array(Array(0, Will, 33, 385))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map((x:String) => x.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(Int, Int)\r\n",
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:33\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line: String)={\n",
    "    val fields = line.split(\",\")\n",
    "    val age = fields(2).toInt\n",
    "    val numFriends = fields(3).toInt\n",
    "    (age,numFriends)\n",
    "}\n",
    "\n",
    "val rdd = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(Int, Int)] = Array((33,385), (26,2), (55,221), (40,465), (68,21))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(Int, Int)] = Array((33,385), (26,2), (55,221), (40,465), (68,21), (59,318), (37,220), (54,307), (38,380), (27,181), (53,191), (57,372), (54,253), (56,444), (43,49), (36,49), (22,323), (35,13), (45,455), (60,246), (67,220), (19,268), (30,72), (51,271), (25,1), (21,445), (22,100), (42,363), (49,476), (48,364), (50,175), (39,161), (26,281), (53,197), (43,249), (27,305), (32,81), (58,21), (64,65), (31,192), (52,413), (67,167), (54,75), (58,345), (35,244), (52,77), (25,96), (24,49), (20,1), (40,254), (51,283), (36,212), (19,269), (62,31), (19,5), (41,278), (44,194), (57,294), (59,158), (59,284), (20,100), (62,442), (69,9), (58,54), (31,15), (52,169), (21,477), (48,135), (33,74), (30,204), (52,393), (45,184), (22,179), (20,384), (65,208), (40,459), (62,201), (40,407), (61,337), (..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(x => x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(Int, (Int, Int))] = Array((33,(385,1)), (26,(2,1)), (55,(221,1)), (40,(465,1)), (68,(21,1)), (59,(318,1)), (37,(220,1)), (54,(307,1)), (38,(380,1)), (27,(181,1)), (53,(191,1)), (57,(372,1)), (54,(253,1)), (56,(444,1)), (43,(49,1)), (36,(49,1)), (22,(323,1)), (35,(13,1)), (45,(455,1)), (60,(246,1)), (67,(220,1)), (19,(268,1)), (30,(72,1)), (51,(271,1)), (25,(1,1)), (21,(445,1)), (22,(100,1)), (42,(363,1)), (49,(476,1)), (48,(364,1)), (50,(175,1)), (39,(161,1)), (26,(281,1)), (53,(197,1)), (43,(249,1)), (27,(305,1)), (32,(81,1)), (58,(21,1)), (64,(65,1)), (31,(192,1)), (52,(413,1)), (67,(167,1)), (54,(75,1)), (58,(345,1)), (35,(244,1)), (52,(77,1)), (25,(96,1)), (24,(49,1)), (20,(1,1)), (40,(254,1)), (51,(283,1)), (36,(212,1)), (19,(269,1)), (62,(31,1)), (19,(5,1)), (41,(278..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(x => (x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalsByAge: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[8] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalsByAge = rdd.mapValues(x => (x,1)).reduceByKey((x,y) => (x._1+y._1,x._2+y._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(Int, (Int, Int))] = Array((34,(1473,6)), (52,(3747,11)), (56,(1840,6)), (66,(2488,9)), (22,(1445,7)), (28,(2091,10)), (54,(3615,13)), (46,(2908,13)), (48,(2814,10)), (30,(2594,11)), (50,(1273,5)), (32,(2287,11)), (36,(2466,10)), (24,(1169,5)), (62,(2870,13)), (64,(3376,12)), (42,(1821,6)), (40,(4264,17)), (18,(2747,8)), (20,(825,5)), (38,(2903,15)), (58,(1282,11)), (44,(3386,12)), (60,(1419,7)), (26,(4115,17)), (68,(2696,10)), (19,(2346,11)), (39,(1185,7)), (41,(2417,9)), (61,(2306,9)), (21,(2807,8)), (47,(2099,9)), (55,(3842,13)), (53,(1560,7)), (25,(2172,11)), (29,(2591,12)), (59,(1980,9)), (65,(1491,5)), (35,(1693,8)), (27,(1825,8)), (57,(3106,12)), (51,(2115,7)), (33,(3904,12)), (37,(2244,9)), (23,(2463,10)), (45,(4024,13)), (63,(1536,4)), (67,(3434,16)), (69,(2352,10))..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalsByAge.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "averageByAge: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[10] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageByAge = totalsByAge.mapValues(x => x._1/x._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(Int, Int)] = Array((18,343), (19,213), (20,165), (21,350), (22,206), (23,246), (24,233), (25,197), (26,242), (27,228), (28,209), (29,215), (30,235), (31,267), (32,207), (33,325), (34,245), (35,211), (36,246), (37,249), (38,193), (39,169), (40,250), (41,268), (42,303), (43,230), (44,282), (45,309), (46,223), (47,233), (48,281), (49,184), (50,254), (51,302), (52,340), (53,222), (54,278), (55,295), (56,306), (57,258), (58,116), (59,220), (60,202), (61,256), (62,220), (63,384), (64,281), (65,298), (66,276), (67,214), (68,269), (69,235))\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByAge.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44,282)(18,343)(19,213)(20,165)(21,350)(45,309)(22,206)(23,246)(24,233)(25,197)(26,242)(27,228)(28,209)(29,215)(30,235)(31,267)(32,207)(33,325)(46,223)(34,245)(47,233)(35,211)(48,281)(36,246)(49,184)(50,254)(51,302)(52,340)(53,222)(54,278)(55,295)(56,306)(57,258)(58,116)(59,220)(60,202)(61,256)(62,220)(63,384)(64,281)(65,298)(66,276)(67,214)(68,269)(69,235)(37,249)(38,193)(39,169)(40,250)(41,268)(42,303)(43,230)"
     ]
    }
   ],
   "source": [
    "averageByAge.sortByKey().foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the minimum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/1800.csv MapPartitionsRDD[24] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/1800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[String] = Array(ITE00100554,18000101,TMAX,-75,,,E,, ITE00100554,18000101,TMIN,-148,,,E,, GM000010962,18000101,PRCP,0,,,E,)\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Double = 32.08999999895692\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.1f * (9.0f / 5.0f) + 32.0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Double = 32.09\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.1 * (9.0 / 5.0) + 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(String, String, Float)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line:String)={\n",
    "    val fields = line.split(\",\")\n",
    "    val stationID = fields(0)\n",
    "    val entryType  = fields(2)\n",
    "    val temperature = fields(3).toFloat * 0.1f * (9.0f / 5.0f ) + 32.0f\n",
    "    (stationID,entryType,temperature)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsedLines: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[25] at map at <console>:28\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedLines = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[(String, String, Float)] = Array((ITE00100554,TMAX,18.5), (ITE00100554,TMIN,5.3600006))\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minTemps: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[26] at filter at <console>:26\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minTemps = parsedLines.filter(x => x._2 == \"TMIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[(String, String, Float)] = Array((ITE00100554,TMIN,5.3600006), (EZE00100082,TMIN,7.700001))\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationTemps: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[27] at map at <console>:26\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationTemps = minTemps.map(x => (x._1,x._3.toFloat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[(String, Float)] = Array((ITE00100554,5.3600006), (EZE00100082,7.700001))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationTemps.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.min\r\n",
       "minTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[28] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.min\n",
    "val minTempsByStation = stationTemps.reduceByKey( (x,y) => min(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[(String, Float)] = Array((EZE00100082,7.700001), (ITE00100554,5.3600006))\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTempsByStation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[(String, Float)] = Array((ITE00100554,5.3600006), (EZE00100082,7.700001), (ITE00100554,9.5), (EZE00100082,8.6), (ITE00100554,23.720001), (EZE00100082,18.86), (ITE00100554,29.66), (EZE00100082,18.68), (ITE00100554,30.92), (EZE00100082,21.56), (ITE00100554,34.34), (EZE00100082,21.74), (ITE00100554,33.8), (EZE00100082,23.0), (ITE00100554,34.52), (EZE00100082,26.42), (ITE00100554,36.14), (EZE00100082,23.720001), (ITE00100554,37.58), (EZE00100082,18.5), (ITE00100554,39.38), (EZE00100082,20.84), (ITE00100554,37.22), (EZE00100082,21.2), (ITE00100554,34.34), (EZE00100082,21.2), (ITE00100554,36.14), (EZE00100082,25.7), (ITE00100554,39.38), (EZE00100082,27.86), (ITE00100554,39.38), (EZE00100082,25.34), (ITE00100554,40.64), (EZE00100082,25.7), (ITE00100554,40.28), (EZE00100082,33.62),..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationTemps.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 --> 7.700001\n",
      "ITE00100554 --> 5.3600006\n"
     ]
    }
   ],
   "source": [
    "minTempsByStation.collect().foreach((x) => println(x._1+\" --> \"+x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the Maximum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 --> 90.14\n",
      "ITE00100554 --> 90.14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.math.max\r\n",
       "maxTemps: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[29] at filter at <console>:31\r\n",
       "stationTemps: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[30] at map at <console>:32\r\n",
       "maxTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[31] at reduceByKey at <console>:33\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.max\n",
    "\n",
    "val maxTemps = parsedLines.filter(x => x._2 == \"TMAX\")\n",
    "val stationTemps = maxTemps.map(x => (x._1,x._3.toFloat))\n",
    "val maxTempsByStation = stationTemps.reduceByKey( (x,y) => max(x,y))\n",
    "maxTempsByStation.collect().foreach((x) => println(x._1+\" --> \"+x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FlatMap vs Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/book.txt MapPartitionsRDD[33] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Employment: Building an Internet Business of One\n",
      "Achieving Financial and Personal Freedom through a Lifestyle Technology Business\n",
      "By Frank Kane\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapLine: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[35] at map at <console>:28\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapLine = lines.map(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[Array[String]] = Array(Array(Self-Employment:, Building, an, Internet, Business, of, One), Array(Achieving, Financial, and, Personal, Freedom, through, a, Lifestyle, Technology, Business), Array(By, Frank, Kane), Array(\"\"))\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Array[String] = Array(Self-Employment:, Building, an, Internet, Business, of, One)\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Int = 7\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)(0).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatmapLine: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at flatMap at <console>:28\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flatmapLine = lines.flatMap(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[String] = Array(Self-Employment:, Building, an, Internet)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[String] = Array(Self-Employment:, Building, an, Internet, Business, of, One, Achieving, Financial, and, Personal, Freedom, through, a, Lifestyle, Technology, Business, By, Frank, Kane, \"\", \"\", \"\", Copyright, ?, 2015, Frank, Kane., All, rights, reserved, worldwide., \"\", \"\", CONTENTS, Disclaimer, Preface, Part, I:, Making, the, Big, Decision, Overcoming, Inertia, Fear, of, Failure, Career, Indoctrination, The, Carrot, on, a, Stick, Ego, Protection, Your, Employer, as, a, Security, Blanket, Why, it?s, Worth, it, Unlimited, Growth, Potential, Investing, in, Yourself,, Not, Someone, Else, No, Dependencies, No, Commute, Freedom, to, Live, Where, You, Want, Freedom, to, Work, When, You, Want, Freedom, to, Work, How, You, Want, Is, Self-Employment, for, You?, Flowchart:, Should, I,..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: scala.collection.Map[String,Long] = Map(foolproof -> 1, precious -> 1, inflammatory -> 1, referrer, -> 1, hourly -> 3, embedded -> 1, way). -> 1, touch, -> 1, of. -> 3, salesperson -> 5, Leeches -> 1, expansion. -> 1, rate -> 7, appropriate. -> 2, CPA?s -> 1, 2014 -> 2, WELL-MEANING -> 1, Talk -> 5, Much -> 1, Builder\" -> 1, plugin -> 3, headache -> 1, purchasing -> 9, China\" -> 1, looks -> 2, site, -> 7, ranking -> 2, scare -> 1, hard-earned -> 1, freedom? -> 1, Seattle, -> 3, PULLING -> 1, action. -> 1, accident -> 3, scale. -> 2, looking. -> 1, physically -> 1, 27, -> 1, call. -> 1, contracts -> 3, twofold. -> 1, scenario -> 1, Advertising -> 3, way? -> 2, nudge -> 1, gamble -> 1, ideas -> 19, sketches -> 1, static -> 1, freelancer. -> 1, ?PR:? -> 1, joining -> 1, particularly..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: scala.collection.Map[String,Long] = Map(foolproof -> 1, precious -> 1, inflammatory -> 1)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.countByValue().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[52] at map at <console>:30\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: scala.collection.Map[String,Long] = Map(serious -> 1, foolproof -> 1, precious -> 2, inflammatory -> 1, hourly -> 3, embedded -> 1, salesperson -> 7, plentiful -> 1, rate -> 9, 2014 -> 8, plugin -> 3, headache -> 1, purchasing -> 9, ons -> 1, bing -> 1, looks -> 2, ranking -> 2, irs -> 3, california -> 2, scare -> 1, finalized -> 1, associations -> 1, accident -> 3, physically -> 2, conversations -> 1, contracts -> 4, scenario -> 1, nudge -> 2, gamble -> 4, ideas -> 27, sketches -> 1, static -> 1, oculus -> 5, unity -> 1, tweeted -> 1, joining -> 1, particularly -> 1, used -> 18, eye -> 3, striking -> 2, minority -> 1, automatic -> 2, widely -> 4, impressions -> 5, checklist -> 3, e -> 1, conversion -> 4, worded -> 1, unidirectional -> 1, significantly -> 4, beautiful -> 1, provi..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase()).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: scala.collection.Map[String,Long] = Map(serious -> 1, foolproof -> 1, precious -> 2, inflammatory -> 1, hourly -> 3, embedded -> 1, salesperson -> 7, plentiful -> 1, rate -> 9, 2014 -> 8, plugin -> 3, headache -> 1, purchasing -> 9, ons -> 1, bing -> 1, looks -> 2, ranking -> 2, irs -> 3, california -> 2, scare -> 1, finalized -> 1, associations -> 1, accident -> 3, physically -> 2, conversations -> 1, contracts -> 4, scenario -> 1, nudge -> 2, gamble -> 4, ideas -> 27, sketches -> 1, static -> 1, oculus -> 5, unity -> 1, tweeted -> 1, joining -> 1, particularly -> 1, used -> 18, eye -> 3, striking -> 2, minority -> 1, automatic -> 2, widely -> 4, impressions -> 5, checklist -> 3, e -> 1, conversion -> 4, worded -> 1, unidirectional -> 1, significantly -> 4, beautiful -> 1, provi..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase()).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[75] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts = lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> (x,1)).reduceByKey( (x,y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedWcounts: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[83] at sortByKey at <console>:28\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedWcounts = wordCounts.map(x => (x._2,x._1)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Array[(Int, String)] = Array((1,transitions), (1,intimately))\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedWcounts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[(Int, String)] = Array((1,transitions), (1,intimately), (1,SEARCH), (1,312), (1,conjure))\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// for (result <- sortedWcounts) {\n",
    "//       val count = result._1\n",
    "//       val word = result._2\n",
    "//       println(s\"$word: $count\")\n",
    "//     }\n",
    "sortedWcounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "lines: Iterator[String] = non-empty iterator\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: String = 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res75: String = 2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res76: Int = 1680\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: Iterator[String] = non-empty iterator\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieNames: Map[Int,String] = Map(5 -> 9, 1 -> 6, 6 -> 9, 9 -> 9, 2 -> 9, 7 -> 9, 3 -> 9, 8 -> 9, 4 -> 9)\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var movieNames : Map[Int,String]=Map()\n",
    "for(line <- lines){\n",
    "    var fields = line.split(\"|\")\n",
    "    if(fields.length > 1){\n",
    "        movieNames += (fields(0).toInt -> fields(1) )\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589277575557)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "import java.nio.charset.CodingErrorAction\r\n",
       "import scala.io.Codec\r\n",
       "loadMoviesName: ()Map[Int,String]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "import java.nio.charset.CodingErrorAction\n",
    "import scala.io.Codec\n",
    "def loadMoviesName(): Map[Int,String] = {\n",
    "    // Handle character encoding issues:\n",
    "    implicit val codec = Codec(\"UTF-8\")\n",
    "    codec.onMalformedInput(CodingErrorAction.REPLACE)\n",
    "    codec.onUnmappableCharacter(CodingErrorAction.REPLACE)\n",
    "    \n",
    "    val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()\n",
    "    var movieNames : Map[Int,String]=Map()\n",
    "    for(line <- lines){\n",
    "        var fields = line.split('|')\n",
    "        if(fields.length > 1){\n",
    "            movieNames += (fields(0).toInt -> fields(1))\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    return movieNames\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n: Map[Int,String] = Map(645 -> Paris Is Burning (1990), 892 -> Flubber (1997), 69 -> Forrest Gump (1994), 1322 -> Metisse (Caf? au Lait) (1993), 1665 -> Brother's Kiss, A (1997), 1036 -> Drop Dead Fred (1991), 1586 -> Lashou shentan (1992), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 809 -> Rising Sun (1993), 1337 -> Larger Than Life (1996), 1411 -> Barbarella (1968), 629 -> Victor/Victoria (1982), 1024 -> Mrs. Dalloway (1997), 1469 -> Tom and Huck (1995), 365 -> Powder (1995), 1369 -> Forbidden Christ, The (Cristo proibito, Il) (1950), 138 -> D3: The Mighty Ducks (1996), 1190 -> That Old Feeling (1997), 1168 -> Little Buddha (1993), 760 -> Screamers (1995), 101 -> Heavy Metal (1981), 1454 -> Angel and the Badman (1947), 1633 -> ? k?ldum klaka (Cold Fever) (1994), 479..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n = loadMoviesName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameDict: org.apache.spark.broadcast.Broadcast[Map[Int,String]] = Broadcast(0)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var nameDict = sc.broadcast(loadMoviesName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/ml-100k/u.data MapPartitionsRDD[1] at textFile at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[2] at map at <console>:27\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = lines.map(x => (x.split(\"\\t\")(1).toInt,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(Int, Int)] = Array((242,1))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesCounts: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[3] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesCounts = movies.reduceByKey((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Int, Int)] = Array((454,16), (1084,21))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesCounts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flipped: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:27\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flipped = moviesCounts.map(x => (x._2,x._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(Int, Int)] = Array((16,454), (21,1084))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesSorted: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[8] at sortByKey at <console>:27\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesSorted = flipped.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(Int, Int)] = Array((1,1494), (1,1414))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesSorted.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\r\n",
       "import org.apache.spark.SparkContext._\r\n",
       "import org.apache.spark._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(ID:Int, name:String, age:Int, numFriends:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapper: (line: String)Person\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(line:String): Person = {\n",
    "    val fields = line.split(',')\n",
    "    val person:Person = Person(fields(0).toInt,fields(1),fields(2).toInt,fields(3).toInt)\n",
    "    return person\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2a0a1b43\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"SparkSQL\").master(\"local[*]\").config(\"spark.sql.warehouse.dir\",\"file:///C:/workspace/spark/hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[76] at textFile at <console>:38\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = spark.sparkContext.textFile(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.Dataset[Person] = [ID: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people = lines.map(mapper).toDS().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our inferred schema:\n",
      "root\n",
      " |-- ID: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- numFriends: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " println(\"Here is our inferred schema:\")\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Array[String] = Array(0,Will,33,385, 1,Jean-Luc,26,2)\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[96] at map at <console>:42\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map(x => x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people = spark.read.csv(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---+\n",
      "|_c0|     _c1|_c2|_c3|\n",
      "+---+--------+---+---+\n",
      "|  0|    Will| 33|385|\n",
      "|  1|Jean-Luc| 26|  2|\n",
      "|  2|    Hugh| 55|221|\n",
      "+---+--------+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[org.apache.spark.sql.Row] = Array([Data], [Beverly], [Hugh], [Dukat], [Jean-Luc], [Nog], [Odo], [Kasidy], [Guinan], [Leeta], [Rom], [Geordi], [Brunt], [Deanna], [Ben], [Weyoun], [Miles], [Will], [Julian], [Lwaxana], [Jadzia], [Ezri], [Keiko], [Elim], [Worf], [Nerys], [Quark], [Martok], [Morn], [Gowron])\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.groupBy(\"_c1\").sum().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### working with class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589341912310)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(ID:Int, name:String, age:Int, numFriends:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person: Person = Person(1,will,33,342)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val person: Person = Person(1,\"will\",33,342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Person = Person(1,will,33,342)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapper: (line: String)Person\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(line:String): Person={\n",
    "    val fields = line.split(',')\n",
    "    val person: Person = Person(fields(0).toInt, fields(1),fields(2).toInt,fields(3).toInt)\n",
    "    return person\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\r\n",
       "import org.apache.spark.SparkContext._\r\n",
       "import org.apache.spark._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5c324076\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark._\n",
    "val spark = SparkSession.builder.appName(\"SparkSQL\").master(\"local[*]\").config(\"spark.sql.warehouse.dir\",\"file:///C:/workspace/spark/hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[21] at textFile at <console>:42\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val lines = spark.sparkContext.textFile(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(ID,IntegerType,true), StructField(name,StringType,true), StructField(age,IntegerType,true), StructField(numFriends,IntegerType,true))\r\n",
       "csv_df: org.apache.spark.sql.DataFrame = [ID: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "  StructField(\"ID\", IntegerType, true),\n",
    "  StructField(\"name\", StringType, true),\n",
    "  StructField(\"age\", IntegerType, true),\n",
    "  StructField(\"numFriends\", IntegerType, true)\n",
    "))\n",
    "\n",
    "val csv_df = spark.read.format(\"csv\").schema(customSchema).load(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| ID|    name|age|numFriends|\n",
      "+---+--------+---+----------+\n",
      "|  0|    Will| 33|       385|\n",
      "|  1|Jean-Luc| 26|         2|\n",
      "|  2|    Hugh| 55|       221|\n",
      "|  3|  Deanna| 40|       465|\n",
      "|  4|   Quark| 68|        21|\n",
      "|  5|  Weyoun| 59|       318|\n",
      "|  6|  Gowron| 37|       220|\n",
      "|  7|    Will| 54|       307|\n",
      "|  8|  Jadzia| 38|       380|\n",
      "|  9|    Hugh| 27|       181|\n",
      "| 10|     Odo| 53|       191|\n",
      "| 11|     Ben| 57|       372|\n",
      "| 12|   Keiko| 54|       253|\n",
      "| 13|Jean-Luc| 56|       444|\n",
      "| 14|    Hugh| 43|        49|\n",
      "| 15|     Rom| 36|        49|\n",
      "| 16|  Weyoun| 22|       323|\n",
      "| 17|     Odo| 35|        13|\n",
      "| 18|Jean-Luc| 45|       455|\n",
      "| 19|  Geordi| 60|       246|\n",
      "+---+--------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: org.apache.spark.sql.Row = [1,Jean-Luc,26,2]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df.rdd.collect()(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Long = 198\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people where numFriends > 300\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    name|age|(age + 10)|\n",
      "+--------+---+----------+\n",
      "|    Will| 33|        43|\n",
      "|Jean-Luc| 26|        36|\n",
      "|    Hugh| 55|        65|\n",
      "|  Deanna| 40|        50|\n",
      "|   Quark| 68|        78|\n",
      "|  Weyoun| 59|        69|\n",
      "|  Gowron| 37|        47|\n",
      "|    Will| 54|        64|\n",
      "|  Jadzia| 38|        48|\n",
      "|    Hugh| 27|        37|\n",
      "|     Odo| 53|        63|\n",
      "|     Ben| 57|        67|\n",
      "|   Keiko| 54|        64|\n",
      "|Jean-Luc| 56|        66|\n",
      "|    Hugh| 43|        53|\n",
      "|     Rom| 36|        46|\n",
      "|  Weyoun| 22|        32|\n",
      "|     Odo| 35|        45|\n",
      "|Jean-Luc| 45|        55|\n",
      "|  Geordi| 60|        70|\n",
      "+--------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT name,age,age+10 FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Will| 33|\n",
      "|    Hugh| 55|\n",
      "|  Deanna| 40|\n",
      "|   Quark| 68|\n",
      "|  Weyoun| 59|\n",
      "|  Gowron| 37|\n",
      "|    Will| 54|\n",
      "|  Jadzia| 38|\n",
      "|     Odo| 53|\n",
      "|     Ben| 57|\n",
      "|   Keiko| 54|\n",
      "|Jean-Luc| 56|\n",
      "|    Hugh| 43|\n",
      "|     Rom| 36|\n",
      "|     Odo| 35|\n",
      "|Jean-Luc| 45|\n",
      "|  Geordi| 60|\n",
      "|     Odo| 67|\n",
      "|   Keiko| 51|\n",
      "|   Leeta| 42|\n",
      "+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.select($\"name\",$\"age\").filter($\"age\" > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Will| 33|\n",
      "|    Hugh| 55|\n",
      "|  Deanna| 40|\n",
      "|   Quark| 68|\n",
      "|  Weyoun| 59|\n",
      "|  Gowron| 37|\n",
      "|    Will| 54|\n",
      "|  Jadzia| 38|\n",
      "|     Odo| 53|\n",
      "|     Ben| 57|\n",
      "|   Keiko| 54|\n",
      "|Jean-Luc| 56|\n",
      "|    Hugh| 43|\n",
      "|     Rom| 36|\n",
      "|     Odo| 35|\n",
      "|Jean-Luc| 45|\n",
      "|  Geordi| 60|\n",
      "|     Odo| 67|\n",
      "|   Keiko| 51|\n",
      "|   Leeta| 42|\n",
      "+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.filter($\"age\" > 30).select($\"name\",$\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| ID|    name|age|numFriends|\n",
      "+---+--------+---+----------+\n",
      "|  0|    Will| 33|       385|\n",
      "|  1|Jean-Luc| 26|         2|\n",
      "|  2|    Hugh| 55|       221|\n",
      "+---+--------+---+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[1] at textFile at <console>:31\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: Array[String] = Array(0,Will,33,385, 1,Jean-Luc,26,2)\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: Array[String] = Array(0,Will,33,385, 1,Jean-Luc,26,2, 2,Hugh,55,221, 3,Deanna,40,465, 4,Quark,68,21, 5,Weyoun,59,318, 6,Gowron,37,220, 7,Will,54,307, 8,Jadzia,38,380, 9,Hugh,27,181, 10,Odo,53,191, 11,Ben,57,372, 12,Keiko,54,253, 13,Jean-Luc,56,444, 14,Hugh,43,49, 15,Rom,36,49, 16,Weyoun,22,323, 17,Odo,35,13, 18,Jean-Luc,45,455, 19,Geordi,60,246, 20,Odo,67,220, 21,Miles,19,268, 22,Quark,30,72, 23,Keiko,51,271, 24,Julian,25,1, 25,Ben,21,445, 26,Julian,22,100, 27,Leeta,42,363, 28,Martok,49,476, 29,Nog,48,364, 30,Keiko,50,175, 31,Miles,39,161, 32,Nog,26,281, 33,Dukat,53,197, 34,Jean-Luc,43,249, 35,Beverly,27,305, 36,Kasidy,32,81, 37,Geordi,58,21, 38,Deanna,64,65, 39,Morn,31,192, 40,Odo,52,413, 41,Hugh,67,167, 42,Brunt,54,75, 43,Guinan,58,345, 44,Nerys,35,244, 45,Dukat,52,77, 46,Morn,..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "//rdd.map(x => x.split(\",\")).collect() ----- aome ide issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scala> val rdd = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\")\n",
    "rdd: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[1] at textFile at <console>:24\n",
    "\n",
    "scala> rdd.map(x => x.split(\",\")).collect()\n",
    "res0: Array[Array[String]] = Array(Array(0, Will, 33, 385), Array(1, Jean-Luc, 26, 2), Array(2, Hugh, 55, 221), Array(3, Deanna, 40, 465), Array(4, Quark, 68, 21), Array(5, Weyoun, 59, 318), Array(6, Gowron, 37, 220), Array(7, Will, 54, 307), Array(8, Jadzia, 38, 380), Array(9, Hugh, 27, 181), Array(10, Odo, 53, 191), Array(11, Ben, 57, 372), Array(12, Keiko, 54, 253), Array(13, Jean-Luc, 56, 444), Array(14, Hugh, 43, 49), Array(15, Rom, 36, 49), Array(16, Weyoun, 22, 323), Array(17, Odo, 35, 13), Array(18, Jean-Luc, 45, 455), Array(19, Geordi, 60, 246), Array(20, Odo, 67, 220), Array(21, Miles, 19, 268), Array(22, Quark, 30, 72), Array(23, Keiko, 51, 271), Array(24, Julian, 25, 1), Array(25, Ben, 21, 445), Array(26, Julian, 22, 100), Array(27, Leeta, 42, 363), Array(28, Martok, 49, 476...\n",
    "scala>\n",
    "\n",
    "scala> case class Person(ID:Int,name:String, age:Int, numFriends:Int)\n",
    "defined class Person\n",
    "\n",
    "scala> rdd.map(x => x.split(\",\")).map(x => Person(x(0).toInt, x(1),x(2).toInt, x(3).toInt) )\n",
    "res1: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[4] at map at <console>:28\n",
    "\n",
    "scala> val rdd2df = rdd.map(x => x.split(\",\")).map(x => Person(x(0).toInt, x(1),x(2).toInt, x(3).toInt))\n",
    "rdd2df: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[6] at map at <console>:27\n",
    "\n",
    "scala> rdd2df.take(3)\n",
    "res2: Array[Person] = Array(Person(0,Will,33,385), Person(1,Jean-Luc,26,2), Person(2,Hugh,55,221))\n",
    "\n",
    "scala> rdd2df.toDS()\n",
    "res3: org.apache.spark.sql.Dataset[Person] = [ID: int, name: string ... 2 more fields]\n",
    "\n",
    "scala> rdd2df.toDS().take(2)\n",
    "res4: Array[Person] = Array(Person(0,Will,33,385), Person(1,Jean-Luc,26,2))\n",
    "\n",
    "scala> val data = rdd2df.toDS().cache()\n",
    "data: org.apache.spark.sql.Dataset[Person] = [ID: int, name: string ... 2 more fields]\n",
    "\n",
    "\n",
    "scala> data.show(1)\n",
    "+---+----+---+----------+\n",
    "| ID|name|age|numFriends|\n",
    "+---+----+---+----------+\n",
    "|  0|Will| 33|       385|\n",
    "+---+----+---+----------+\n",
    "only showing top 1 row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| ID|    name|age|numFriends|\n",
      "+---+--------+---+----------+\n",
      "|  0|    Will| 33|       385|\n",
      "|  1|Jean-Luc| 26|         2|\n",
      "|  2|    Hugh| 55|       221|\n",
      "|  3|  Deanna| 40|       465|\n",
      "|  4|   Quark| 68|        21|\n",
      "|  5|  Weyoun| 59|       318|\n",
      "|  6|  Gowron| 37|       220|\n",
      "|  7|    Will| 54|       307|\n",
      "|  8|  Jadzia| 38|       380|\n",
      "|  9|    Hugh| 27|       181|\n",
      "| 10|     Odo| 53|       191|\n",
      "| 11|     Ben| 57|       372|\n",
      "| 12|   Keiko| 54|       253|\n",
      "| 13|Jean-Luc| 56|       444|\n",
      "| 14|    Hugh| 43|        49|\n",
      "| 15|     Rom| 36|        49|\n",
      "| 16|  Weyoun| 22|       323|\n",
      "| 17|     Odo| 35|        13|\n",
      "| 18|Jean-Luc| 45|       455|\n",
      "| 19|  Geordi| 60|       246|\n",
      "+---+--------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=====================================================`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[Array[String]] = Array(Array(0, Will, 33, 385), Array(1, Jean-Luc, 26, 2), Array(2, Hugh, 55, 221), Array(3, Deanna, 40, 465), Array(4, Quark, 68, 21), Array(5, Weyoun, 59, 318), Array(6, Gowron, 37, 220), Array(7, Will, 54, 307), Array(8, Jadzia, 38, 380), Array(9, Hugh, 27, 181), Array(10, Odo, 53, 191), Array(11, Ben, 57, 372), Array(12, Keiko, 54, 253), Array(13, Jean-Luc, 56, 444), Array(14, Hugh, 43, 49), Array(15, Rom, 36, 49), Array(16, Weyoun, 22, 323), Array(17, Odo, 35, 13), Array(18, Jean-Luc, 45, 455), Array(19, Geordi, 60, 246), Array(20, Odo, 67, 220), Array(21, Miles, 19, 268), Array(22, Quark, 30, 72), Array(23, Keiko, 51, 271), Array(24, Julian, 25, 1), Array(25, Ben, 21, 445), Array(26, Julian, 22, 100), Array(27, Leeta, 42, 363), Array(28, Martok, 49, 476..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(x => x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BroadCast: org.apache.spark.broadcast.Broadcast[Array[Array[String]]] = Broadcast(4)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val BroadCast = sc.broadcast(rdd.map(x => x.split(\",\")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[Array[String]] = Array(Array(0, Will, 33, 385), Array(1, Jean-Luc, 26, 2), Array(2, Hugh, 55, 221), Array(3, Deanna, 40, 465), Array(4, Quark, 68, 21), Array(5, Weyoun, 59, 318), Array(6, Gowron, 37, 220), Array(7, Will, 54, 307), Array(8, Jadzia, 38, 380), Array(9, Hugh, 27, 181), Array(10, Odo, 53, 191), Array(11, Ben, 57, 372), Array(12, Keiko, 54, 253), Array(13, Jean-Luc, 56, 444), Array(14, Hugh, 43, 49), Array(15, Rom, 36, 49), Array(16, Weyoun, 22, 323), Array(17, Odo, 35, 13), Array(18, Jean-Luc, 45, 455), Array(19, Geordi, 60, 246), Array(20, Odo, 67, 220), Array(21, Miles, 19, 268), Array(22, Quark, 30, 72), Array(23, Keiko, 51, 271), Array(24, Julian, 25, 1), Array(25, Ben, 21, 445), Array(26, Julian, 22, 100), Array(27, Leeta, 42, 363), Array(28, Martok, 49, 476..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BroadCast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589359729637)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext._\r\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fakefriendsdf: org.apache.spark.sql.DataFrame = [value: array<string>]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fakefriendsdf=rdd.map(x=> x.split(\",\")).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.Row = [WrappedArray(0, Will, 33, 385)]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakefriendsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|  [0, Will, 33, 385]|\n",
      "|[1, Jean-Luc, 26, 2]|\n",
      "|  [2, Hugh, 55, 221]|\n",
      "|[3, Deanna, 40, 465]|\n",
      "|  [4, Quark, 68, 21]|\n",
      "|[5, Weyoun, 59, 318]|\n",
      "|[6, Gowron, 37, 220]|\n",
      "|  [7, Will, 54, 307]|\n",
      "|[8, Jadzia, 38, 380]|\n",
      "|  [9, Hugh, 27, 181]|\n",
      "|  [10, Odo, 53, 191]|\n",
      "|  [11, Ben, 57, 372]|\n",
      "|[12, Keiko, 54, 253]|\n",
      "|[13, Jean-Luc, 56...|\n",
      "|  [14, Hugh, 43, 49]|\n",
      "|   [15, Rom, 36, 49]|\n",
      "|[16, Weyoun, 22, ...|\n",
      "|   [17, Odo, 35, 13]|\n",
      "|[18, Jean-Luc, 45...|\n",
      "|[19, Geordi, 60, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.map(x=> x.split(\",\")).toDS().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
