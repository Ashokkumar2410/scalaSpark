{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589192773996)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(# scalaSpark)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sourceFile: String = C:/workspace/workplace/scala/SparkScala/fakefriends.csv\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sourceFile = \"C:/workspace/workplace/scala/SparkScala/fakefriends.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/fakefriends.csv MapPartitionsRDD[1] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(sourceFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[String] = Array(0,Will,33,385)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[Array[String]] = Array(Array(0, Will, 33, 385))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.map((x:String) => x.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(Int, Int)\r\n",
       "rdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:33\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line: String)={\n",
    "    val fields = line.split(\",\")\n",
    "    val age = fields(2).toInt\n",
    "    val numFriends = fields(3).toInt\n",
    "    (age,numFriends)\n",
    "}\n",
    "\n",
    "val rdd = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(Int, Int)] = Array((33,385), (26,2), (55,221), (40,465), (68,21))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(Int, Int)] = Array((33,385), (26,2), (55,221), (40,465), (68,21), (59,318), (37,220), (54,307), (38,380), (27,181), (53,191), (57,372), (54,253), (56,444), (43,49), (36,49), (22,323), (35,13), (45,455), (60,246), (67,220), (19,268), (30,72), (51,271), (25,1), (21,445), (22,100), (42,363), (49,476), (48,364), (50,175), (39,161), (26,281), (53,197), (43,249), (27,305), (32,81), (58,21), (64,65), (31,192), (52,413), (67,167), (54,75), (58,345), (35,244), (52,77), (25,96), (24,49), (20,1), (40,254), (51,283), (36,212), (19,269), (62,31), (19,5), (41,278), (44,194), (57,294), (59,158), (59,284), (20,100), (62,442), (69,9), (58,54), (31,15), (52,169), (21,477), (48,135), (33,74), (30,204), (52,393), (45,184), (22,179), (20,384), (65,208), (40,459), (62,201), (40,407), (61,337), (..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(x => x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(Int, (Int, Int))] = Array((33,(385,1)), (26,(2,1)), (55,(221,1)), (40,(465,1)), (68,(21,1)), (59,(318,1)), (37,(220,1)), (54,(307,1)), (38,(380,1)), (27,(181,1)), (53,(191,1)), (57,(372,1)), (54,(253,1)), (56,(444,1)), (43,(49,1)), (36,(49,1)), (22,(323,1)), (35,(13,1)), (45,(455,1)), (60,(246,1)), (67,(220,1)), (19,(268,1)), (30,(72,1)), (51,(271,1)), (25,(1,1)), (21,(445,1)), (22,(100,1)), (42,(363,1)), (49,(476,1)), (48,(364,1)), (50,(175,1)), (39,(161,1)), (26,(281,1)), (53,(197,1)), (43,(249,1)), (27,(305,1)), (32,(81,1)), (58,(21,1)), (64,(65,1)), (31,(192,1)), (52,(413,1)), (67,(167,1)), (54,(75,1)), (58,(345,1)), (35,(244,1)), (52,(77,1)), (25,(96,1)), (24,(49,1)), (20,(1,1)), (40,(254,1)), (51,(283,1)), (36,(212,1)), (19,(269,1)), (62,(31,1)), (19,(5,1)), (41,(278..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(x => (x,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totalsByAge: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[8] at reduceByKey at <console>:26\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val totalsByAge = rdd.mapValues(x => (x,1)).reduceByKey((x,y) => (x._1+y._1,x._2+y._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(Int, (Int, Int))] = Array((34,(1473,6)), (52,(3747,11)), (56,(1840,6)), (66,(2488,9)), (22,(1445,7)), (28,(2091,10)), (54,(3615,13)), (46,(2908,13)), (48,(2814,10)), (30,(2594,11)), (50,(1273,5)), (32,(2287,11)), (36,(2466,10)), (24,(1169,5)), (62,(2870,13)), (64,(3376,12)), (42,(1821,6)), (40,(4264,17)), (18,(2747,8)), (20,(825,5)), (38,(2903,15)), (58,(1282,11)), (44,(3386,12)), (60,(1419,7)), (26,(4115,17)), (68,(2696,10)), (19,(2346,11)), (39,(1185,7)), (41,(2417,9)), (61,(2306,9)), (21,(2807,8)), (47,(2099,9)), (55,(3842,13)), (53,(1560,7)), (25,(2172,11)), (29,(2591,12)), (59,(1980,9)), (65,(1491,5)), (35,(1693,8)), (27,(1825,8)), (57,(3106,12)), (51,(2115,7)), (33,(3904,12)), (37,(2244,9)), (23,(2463,10)), (45,(4024,13)), (63,(1536,4)), (67,(3434,16)), (69,(2352,10))..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalsByAge.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "averageByAge: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[10] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageByAge = totalsByAge.mapValues(x => x._1/x._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[(Int, Int)] = Array((18,343), (19,213), (20,165), (21,350), (22,206), (23,246), (24,233), (25,197), (26,242), (27,228), (28,209), (29,215), (30,235), (31,267), (32,207), (33,325), (34,245), (35,211), (36,246), (37,249), (38,193), (39,169), (40,250), (41,268), (42,303), (43,230), (44,282), (45,309), (46,223), (47,233), (48,281), (49,184), (50,254), (51,302), (52,340), (53,222), (54,278), (55,295), (56,306), (57,258), (58,116), (59,220), (60,202), (61,256), (62,220), (63,384), (64,281), (65,298), (66,276), (67,214), (68,269), (69,235))\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByAge.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44,282)(18,343)(19,213)(20,165)(21,350)(45,309)(22,206)(23,246)(24,233)(25,197)(26,242)(27,228)(28,209)(29,215)(30,235)(31,267)(32,207)(33,325)(46,223)(34,245)(47,233)(35,211)(48,281)(36,246)(49,184)(50,254)(51,302)(52,340)(53,222)(54,278)(55,295)(56,306)(57,258)(58,116)(59,220)(60,202)(61,256)(62,220)(63,384)(64,281)(65,298)(66,276)(67,214)(68,269)(69,235)(37,249)(38,193)(39,169)(40,250)(41,268)(42,303)(43,230)"
     ]
    }
   ],
   "source": [
    "averageByAge.sortByKey().foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the minimum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/1800.csv MapPartitionsRDD[24] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/1800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[String] = Array(ITE00100554,18000101,TMAX,-75,,,E,, ITE00100554,18000101,TMIN,-148,,,E,, GM000010962,18000101,PRCP,0,,,E,)\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Double = 32.08999999895692\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.1f * (9.0f / 5.0f) + 32.0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Double = 32.09\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.1 * (9.0 / 5.0) + 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLine: (line: String)(String, String, Float)\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseLine(line:String)={\n",
    "    val fields = line.split(\",\")\n",
    "    val stationID = fields(0)\n",
    "    val entryType  = fields(2)\n",
    "    val temperature = fields(3).toFloat * 0.1f * (9.0f / 5.0f ) + 32.0f\n",
    "    (stationID,entryType,temperature)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsedLines: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[25] at map at <console>:28\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedLines = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[(String, String, Float)] = Array((ITE00100554,TMAX,18.5), (ITE00100554,TMIN,5.3600006))\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minTemps: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[26] at filter at <console>:26\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minTemps = parsedLines.filter(x => x._2 == \"TMIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[(String, String, Float)] = Array((ITE00100554,TMIN,5.3600006), (EZE00100082,TMIN,7.700001))\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationTemps: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[27] at map at <console>:26\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationTemps = minTemps.map(x => (x._1,x._3.toFloat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Array[(String, Float)] = Array((ITE00100554,5.3600006), (EZE00100082,7.700001))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationTemps.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.math.min\r\n",
       "minTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[28] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.min\n",
    "val minTempsByStation = stationTemps.reduceByKey( (x,y) => min(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[(String, Float)] = Array((EZE00100082,7.700001), (ITE00100554,5.3600006))\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTempsByStation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Array[(String, Float)] = Array((ITE00100554,5.3600006), (EZE00100082,7.700001), (ITE00100554,9.5), (EZE00100082,8.6), (ITE00100554,23.720001), (EZE00100082,18.86), (ITE00100554,29.66), (EZE00100082,18.68), (ITE00100554,30.92), (EZE00100082,21.56), (ITE00100554,34.34), (EZE00100082,21.74), (ITE00100554,33.8), (EZE00100082,23.0), (ITE00100554,34.52), (EZE00100082,26.42), (ITE00100554,36.14), (EZE00100082,23.720001), (ITE00100554,37.58), (EZE00100082,18.5), (ITE00100554,39.38), (EZE00100082,20.84), (ITE00100554,37.22), (EZE00100082,21.2), (ITE00100554,34.34), (EZE00100082,21.2), (ITE00100554,36.14), (EZE00100082,25.7), (ITE00100554,39.38), (EZE00100082,27.86), (ITE00100554,39.38), (EZE00100082,25.34), (ITE00100554,40.64), (EZE00100082,25.7), (ITE00100554,40.28), (EZE00100082,33.62),..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationTemps.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 --> 7.700001\n",
      "ITE00100554 --> 5.3600006\n"
     ]
    }
   ],
   "source": [
    "minTempsByStation.collect().foreach((x) => println(x._1+\" --> \"+x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the Maximum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZE00100082 --> 90.14\n",
      "ITE00100554 --> 90.14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.math.max\r\n",
       "maxTemps: org.apache.spark.rdd.RDD[(String, String, Float)] = MapPartitionsRDD[29] at filter at <console>:31\r\n",
       "stationTemps: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[30] at map at <console>:32\r\n",
       "maxTempsByStation: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[31] at reduceByKey at <console>:33\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.max\n",
    "\n",
    "val maxTemps = parsedLines.filter(x => x._2 == \"TMAX\")\n",
    "val stationTemps = maxTemps.map(x => (x._1,x._3.toFloat))\n",
    "val maxTempsByStation = stationTemps.reduceByKey( (x,y) => max(x,y))\n",
    "maxTempsByStation.collect().foreach((x) => println(x._1+\" --> \"+x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FlatMap vs Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/book.txt MapPartitionsRDD[33] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Employment: Building an Internet Business of One\n",
      "Achieving Financial and Personal Freedom through a Lifestyle Technology Business\n",
      "By Frank Kane\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapLine: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[35] at map at <console>:28\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapLine = lines.map(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[Array[String]] = Array(Array(Self-Employment:, Building, an, Internet, Business, of, One), Array(Achieving, Financial, and, Personal, Freedom, through, a, Lifestyle, Technology, Business), Array(By, Frank, Kane), Array(\"\"))\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Array[String] = Array(Self-Employment:, Building, an, Internet, Business, of, One)\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res45: Int = 7\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapLine.take(4)(0).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatmapLine: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at flatMap at <console>:28\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flatmapLine = lines.flatMap(x => x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[String] = Array(Self-Employment:, Building, an, Internet)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Array[String] = Array(Self-Employment:, Building, an, Internet, Business, of, One, Achieving, Financial, and, Personal, Freedom, through, a, Lifestyle, Technology, Business, By, Frank, Kane, \"\", \"\", \"\", Copyright, ?, 2015, Frank, Kane., All, rights, reserved, worldwide., \"\", \"\", CONTENTS, Disclaimer, Preface, Part, I:, Making, the, Big, Decision, Overcoming, Inertia, Fear, of, Failure, Career, Indoctrination, The, Carrot, on, a, Stick, Ego, Protection, Your, Employer, as, a, Security, Blanket, Why, it?s, Worth, it, Unlimited, Growth, Potential, Investing, in, Yourself,, Not, Someone, Else, No, Dependencies, No, Commute, Freedom, to, Live, Where, You, Want, Freedom, to, Work, When, You, Want, Freedom, to, Work, How, You, Want, Is, Self-Employment, for, You?, Flowchart:, Should, I,..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: scala.collection.Map[String,Long] = Map(foolproof -> 1, precious -> 1, inflammatory -> 1, referrer, -> 1, hourly -> 3, embedded -> 1, way). -> 1, touch, -> 1, of. -> 3, salesperson -> 5, Leeches -> 1, expansion. -> 1, rate -> 7, appropriate. -> 2, CPA?s -> 1, 2014 -> 2, WELL-MEANING -> 1, Talk -> 5, Much -> 1, Builder\" -> 1, plugin -> 3, headache -> 1, purchasing -> 9, China\" -> 1, looks -> 2, site, -> 7, ranking -> 2, scare -> 1, hard-earned -> 1, freedom? -> 1, Seattle, -> 3, PULLING -> 1, action. -> 1, accident -> 3, scale. -> 2, looking. -> 1, physically -> 1, 27, -> 1, call. -> 1, contracts -> 3, twofold. -> 1, scenario -> 1, Advertising -> 3, way? -> 2, nudge -> 1, gamble -> 1, ideas -> 19, sketches -> 1, static -> 1, freelancer. -> 1, ?PR:? -> 1, joining -> 1, particularly..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: scala.collection.Map[String,Long] = Map(foolproof -> 1, precious -> 1, inflammatory -> 1)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmapLine.countByValue().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res58: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[52] at map at <console>:30\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: scala.collection.Map[String,Long] = Map(serious -> 1, foolproof -> 1, precious -> 2, inflammatory -> 1, hourly -> 3, embedded -> 1, salesperson -> 7, plentiful -> 1, rate -> 9, 2014 -> 8, plugin -> 3, headache -> 1, purchasing -> 9, ons -> 1, bing -> 1, looks -> 2, ranking -> 2, irs -> 3, california -> 2, scare -> 1, finalized -> 1, associations -> 1, accident -> 3, physically -> 2, conversations -> 1, contracts -> 4, scenario -> 1, nudge -> 2, gamble -> 4, ideas -> 27, sketches -> 1, static -> 1, oculus -> 5, unity -> 1, tweeted -> 1, joining -> 1, particularly -> 1, used -> 18, eye -> 3, striking -> 2, minority -> 1, automatic -> 2, widely -> 4, impressions -> 5, checklist -> 3, e -> 1, conversion -> 4, worded -> 1, unidirectional -> 1, significantly -> 4, beautiful -> 1, provi..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase()).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: scala.collection.Map[String,Long] = Map(serious -> 1, foolproof -> 1, precious -> 2, inflammatory -> 1, hourly -> 3, embedded -> 1, salesperson -> 7, plentiful -> 1, rate -> 9, 2014 -> 8, plugin -> 3, headache -> 1, purchasing -> 9, ons -> 1, bing -> 1, looks -> 2, ranking -> 2, irs -> 3, california -> 2, scare -> 1, finalized -> 1, associations -> 1, accident -> 3, physically -> 2, conversations -> 1, contracts -> 4, scenario -> 1, nudge -> 2, gamble -> 4, ideas -> 27, sketches -> 1, static -> 1, oculus -> 5, unity -> 1, tweeted -> 1, joining -> 1, particularly -> 1, used -> 18, eye -> 3, striking -> 2, minority -> 1, automatic -> 2, widely -> 4, impressions -> 5, checklist -> 3, e -> 1, conversion -> 4, worded -> 1, unidirectional -> 1, significantly -> 4, beautiful -> 1, provi..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> x.toLowerCase()).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[75] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts = lines.flatMap(x => x.split(\"\\\\W+\")).map(x=> (x,1)).reduceByKey( (x,y) => x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedWcounts: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[83] at sortByKey at <console>:28\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sortedWcounts = wordCounts.map(x => (x._2,x._1)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Array[(Int, String)] = Array((1,transitions), (1,intimately))\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedWcounts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[(Int, String)] = Array((1,transitions), (1,intimately), (1,SEARCH), (1,312), (1,conjure))\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// for (result <- sortedWcounts) {\n",
    "//       val count = result._1\n",
    "//       val word = result._2\n",
    "//       println(s\"$word: $count\")\n",
    "//     }\n",
    "sortedWcounts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "lines: Iterator[String] = non-empty iterator\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: String = 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res75: String = 2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res76: Int = 1680\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: Iterator[String] = non-empty iterator\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieNames: Map[Int,String] = Map(5 -> 9, 1 -> 6, 6 -> 9, 9 -> 9, 2 -> 9, 7 -> 9, 3 -> 9, 8 -> 9, 4 -> 9)\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var movieNames : Map[Int,String]=Map()\n",
    "for(line <- lines){\n",
    "    var fields = line.split(\"|\")\n",
    "    if(fields.length > 1){\n",
    "        movieNames += (fields(0).toInt -> fields(1) )\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-7M17AKF:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1589276732530)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\r\n",
       "loadMoviesName: ()Map[Int,String]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "def loadMoviesName(): Map[Int,String] = {\n",
    "    val lines = Source.fromFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.item\").getLines()\n",
    "    var movieNames : Map[Int,String]=Map()\n",
    "    for(line <- lines){\n",
    "        var fields = line.split('|')\n",
    "        if(fields.length > 1){\n",
    "            movieNames += (fields(0).toInt -> fields(1))\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    return movieNames\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val n = loadMoviesName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameDict: org.apache.spark.broadcast.Broadcast[Map[Int,String]] = Broadcast(0)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var nameDict = sc.broadcast(loadMoviesName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = C:/workspace/workplace/scala/SparkScala/ml-100k/u.data MapPartitionsRDD[1] at textFile at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"C:/workspace/workplace/scala/SparkScala/ml-100k/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[2] at map at <console>:27\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = lines.map(x => (x.split(\"\\t\")(1).toInt,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(Int, Int)] = Array((242,1))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesCounts: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[3] at reduceByKey at <console>:27\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesCounts = movies.reduceByKey((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Int, Int)] = Array((454,16), (1084,21))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesCounts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flipped: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[4] at map at <console>:27\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flipped = moviesCounts.map(x => (x._2,x._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(Int, Int)] = Array((16,454), (21,1084))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moviesSorted: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[8] at sortByKey at <console>:27\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val moviesSorted = flipped.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(Int, Int)] = Array((1,1494), (1,1414))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesSorted.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "27: error: not found: value moviesSorted\r",
     "output_type": "error",
     "traceback": [
      "<console>:27: error: not found: value moviesSorted\r",
      "       val sortedMoviesWithNames = moviesSorted.map( x  => (nameDict.value(x._2), x._1) )\r",
      "                                   ^",
      ""
     ]
    }
   ],
   "source": [
    "val sortedMoviesWithNames = moviesSorted.map( x  => (nameDict.value(x._2), x._1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
